{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Lambda\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import os, shutil\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "img_size = 150\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = 'dataset'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = 'datatrain'\n",
    "os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_fire_dir = os.path.join(train_dir, 'fire')\n",
    "os.mkdir(train_fire_dir)\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_normal_dir = os.path.join(train_dir, 'normal')\n",
    "os.mkdir(train_normal_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_fire_dir = os.path.join(validation_dir, 'fire')\n",
    "os.mkdir(validation_fire_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_normal_dir = os.path.join(validation_dir, 'normal')\n",
    "os.mkdir(validation_normal_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "test_fire_dir = os.path.join(test_dir, 'fire')\n",
    "os.mkdir(test_fire_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "test_normal_dir = os.path.join(test_dir, 'normal')\n",
    "os.mkdir(test_normal_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['fire_{}.jpg'.format(i) for i in range(1,600)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_fire_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['fire_{}.jpg'.format(i) for i in range(600, 750)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_fire_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['fire_{}.jpg'.format(i) for i in range(750, 900)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_fire_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['normal_{}.jpg'.format(i) for i in range(1,600)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_normal_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['normal_{}.jpg'.format(i) for i in range(600, 750)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_normal_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['normal_{}.jpg'.format(i) for i in range(750, 900)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_normal_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training fire images:', len(os.listdir(train_fire_dir)))\n",
    "print('total training normal images:', len(os.listdir(train_normal_dir)))\n",
    "print('total validation fire images:', len(os.listdir(validation_fire_dir)))\n",
    "print('total validation normal images:', len(os.listdir(validation_normal_dir)))\n",
    "print('total test fire images:', len(os.listdir(test_fire_dir)))\n",
    "print('total test normal images:', len(os.listdir(test_normal_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "      rescale=1./255,  \n",
    "      rotation_range=20,\n",
    "      #width_shift_range=0.2,\n",
    "      #height_shift_range=0.2,\n",
    "      #shear_range=0.2,\n",
    "      #zoom_range=0.2,\n",
    "      horizontal_flip=True)\n",
    "      #fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    #width_shift_range=0.2,\n",
    "    #height_shift_range=0.2,\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=img_size,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=img_size,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is module with image preprocessing utilities\n",
    "from keras.preprocessing import image\n",
    "\n",
    "fnames = [os.path.join(train_fire_dir, fname) for fname in os.listdir(train_fire_dir)]\n",
    "\n",
    "# We pick one image to \"augment\"\n",
    "import random\n",
    "nomer=random.randint(0,600)\n",
    "\n",
    "img_path = fnames[nomer]\n",
    "\n",
    "# Read the image and resize it\n",
    "img = image.load_img(img_path, target_size=(img_size, img_size))\n",
    "\n",
    "# Convert it to a Numpy array with shape (150, 150, 3)\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# Reshape it to (1, 150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "# The .flow() command below generates batches of randomly transformed images.\n",
    "# It will loop indefinitely, so we need to `break` the loop at some point!\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (img_size, img_size, 3)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=img_shape,weights=\"imagenet\",include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def build_mobilenetv2_model(base_model):\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "          Conv2D(32, 3, activation='relu'),\n",
    "          GlobalAveragePooling2D(),\n",
    "        Dense(126, activation='relu'),\n",
    "        Dense(2, activation='softmax')]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_mobilenetv2_model(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "mc = ModelCheckpoint('fire_train.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    #steps_per_epoch=10,\n",
    "                    epochs=250,\n",
    "                    validation_data=validation_generator,\n",
    "                    #validation_steps=10,\n",
    "                    shuffle = True,\n",
    "                    callbacks=[mc]\n",
    "                  )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "mc = ModelCheckpoint('fire_train_tune.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    #steps_per_epoch=10,\n",
    "                    epochs=50,\n",
    "                    validation_data=validation_generator,\n",
    "                    #validation_steps=10,\n",
    "                    shuffle = True,\n",
    "                    callbacks=[mc]\n",
    "                  )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "mc = ModelCheckpoint('fire_train.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    #steps_per_epoch=10,\n",
    "                    epochs=100,\n",
    "                    validation_data=validation_generator,\n",
    "                    #validation_steps=10,\n",
    "                    shuffle = True,\n",
    "                    callbacks=[mc]\n",
    "                  )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "#from keras.models import load_model\n",
    "#model = load_model('fire_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'datatrain/test'\n",
    "CATEGORIES = ['fire', 'normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in CATEGORIES :\n",
    "    path = os.path.join(DATADIR, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES :\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try :\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_UNCHANGED)\n",
    "                new_array = cv2.resize(img_array, (img_size, img_size))\n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "create_training_data()\n",
    "random.shuffle(training_data)\n",
    "\n",
    "X = [] #features\n",
    "y = [] #labels\n",
    "\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X).reshape(-1, img_size, img_size,3)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_trainnp = y_train.to_numpy()\n",
    "#y_testnp = y.to_numpy()\n",
    "\n",
    "#y_train= to_categorical(y_trainnp)\n",
    "y_test= to_categorical(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.argmax(y_test, axis=1) \n",
    "y_pred = model.predict_classes(X)\n",
    "#report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "#df_report = pd.DataFrame(report).transpose()\n",
    "#df_report.to_csv('Result/DNN-3Layer_ClassReport.csv')\n",
    "\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "#df_report = pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "y_true = y_test\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "f, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(cm, annot = True, linewidths = 0.2, cmap = \"PuBu\", linecolor=\"white\", fmt=\".0f\",ax = ax)\n",
    "#np.savetxt('Result/DNN-3Layer_covmatrix.csv', cm, delimiter=\";\")\n",
    "\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.draw()\n",
    "#plt.savefig('Result/DNN-3Layer_covmatrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model1 = load_model('fire_train_tune.h5')\n",
    "\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('dataset/fire_1.jpg')\n",
    "img = cv2.resize(img,(img_size,img_size))\n",
    "img = np.reshape(img,[1,img_size,img_size,3])\n",
    "\n",
    "classes = model.predict_classes(img)\n",
    "\n",
    "print (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('dataset/normal_2.jpg')\n",
    "img = cv2.resize(img,(img_size,img_size))\n",
    "img = np.reshape(img,[1,img_size,img_size,3])\n",
    "\n",
    "classes = model.predict_classes(img)\n",
    "\n",
    "print (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
